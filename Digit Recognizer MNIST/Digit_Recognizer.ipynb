{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from numpy import float32\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './digit-recognizer/train.csv'\n",
    "train_data = pd.read_csv(train_path, dtype=np.float32)\n",
    "\n",
    "test_path = './digit-recognizer/test.csv'\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret 'torch.float32' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steve\\OneDrive\\Documents\\IntroToDL\\MLfun\\Digit_Recognizer.ipynb Cell 3\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000002?line=0'>1</a>\u001b[0m X_train \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mto_numpy()\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m28\u001b[39;49m, \u001b[39m28\u001b[39;49m)\u001b[39m.\u001b[39;49mastype(torch\u001b[39m.\u001b[39;49mfloat32) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000002?line=1'>2</a>\u001b[0m X_test \u001b[39m=\u001b[39m test_data\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\u001b[39m.\u001b[39mastype(torch\u001b[39m.\u001b[39mfloat32) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000002?line=3'>4</a>\u001b[0m Y_train \u001b[39m=\u001b[39m train_data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret 'torch.float32' as a data type"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = train_data.drop('label', axis=1).to_numpy().reshape(-1, 28, 28).astype(torch.float32) / 255\n",
    "X_test = test_data.to_numpy().reshape(-1, 28, 28).astype(torch.float32) / 255\n",
    "\n",
    "Y_train = train_data['label']\n",
    "\n",
    "#plt.imshow(X_train[1].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steve\\OneDrive\\Documents\\IntroToDL\\MLfun\\Digit_Recognizer.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000003?line=0'>1</a>\u001b[0m train_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mTensor(X_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000003?line=1'>2</a>\u001b[0m test_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000003?line=2'>3</a>\u001b[0m label_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(Y_train)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "train_tensor = torch.Tensor(X_train)\n",
    "test_tensor = torch.Tensor(X_test)\n",
    "label_tensor = torch.Tensor(Y_train)\n",
    "\n",
    "train_tensor = TensorDataset(train_tensor, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trainloader = DataLoader(train_tensor, batch_size=batch_size)\n",
    "testloader = DataLoader(test_tensor, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 28, 28])\n",
      "torch.Size([16, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for image, label in trainloader:\n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Network(\n",
      "  (convNet): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linearNet): Sequential(\n",
      "    (0): Linear(in_features=6272, out_features=256, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (10): Dropout(p=0.2, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=32, out_features=10, bias=True)\n",
      "    (13): Dropout(p=0.2, inplace=False)\n",
      "    (14): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.convNet = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.linearNet = nn.Sequential(\n",
    "            nn.Linear(7*7*128, 256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convNet(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linearNet(x)\n",
    "        return x\n",
    "        \n",
    "model = Network()\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steve\\OneDrive\\Documents\\IntroToDL\\MLfun\\Digit_Recognizer.ipynb Cell 9\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000008?line=4'>5</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000008?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m model(image)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000008?line=6'>7</a>\u001b[0m loss_out \u001b[39m=\u001b[39m loss(output, label)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000008?line=7'>8</a>\u001b[0m loss_out\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/Digit_Recognizer.ipynb#ch0000008?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\steve\\anaconda3\\envs\\pytorchMain\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\steve\\anaconda3\\envs\\pytorchMain\\lib\\site-packages\\torch\\nn\\modules\\loss.py:211\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnll_loss(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\steve\\anaconda3\\envs\\pytorchMain\\lib\\site-packages\\torch\\nn\\functional.py:2532\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2530\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2531\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2532\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'"
     ]
    }
   ],
   "source": [
    "for batch, (image, label) in enumerate(trainloader):\n",
    "    image, label = image.to(device), label.to(device)\n",
    "    image = image.resize(batch_size, 1, 28, 28)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(image)\n",
    "    loss_out = loss(output, label)\n",
    "    loss_out.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss.item())\n",
    "    break\n",
    "    # if(batch % 100 == 0):\n",
    "    #   print(f\"Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorchMain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dea52dcf60439bbfeb15ff1bd40b703081e3cad41733c2dee57afaa83da4e6a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
