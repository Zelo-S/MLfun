{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "\n",
    "from PIL import Image \n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from glob import glob\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A little dataset/dataloading processing??</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DRIVEDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dir)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.image_dir[index], cv2.IMREAD_COLOR)\n",
    "        image = image / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = image.astype(np.float32)\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "\n",
    "        mask = cv2.imread(self.mask_dir[index], cv2.IMREAD_COLOR)\n",
    "        print(type(mask))\n",
    "        mask = mask / 255.0\n",
    "\n",
    "        #mask = np.expand_dims(mask, axis=0)\n",
    "        mask = np.transpose(mask, (2,1,0))\n",
    "\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask = torch.from_numpy(mask)\n",
    "        \n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_IMAGE_DIR = './datasets/training/training/images'\n",
    "TRAIN_MASK_DIR = './datasets/training/training/true_mask'\n",
    "\n",
    "VALID_IMAGE_DIR = './datasets/training/validation/images'\n",
    "VALID_MASK_DIR = './datasets/training/validation/true_mask'\n",
    "\n",
    "BATCH_SIZE = 2 \n",
    "IMAGE_HEIGHT = 512 \n",
    "IMAGE_WIDTH = 512\n",
    "\n",
    "training_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.1),\n",
    "    A.Rotate(limit=30, p=0.1),\n",
    "    A.Normalize(\n",
    "        mean=[0.0, 0.0, 0.0],\n",
    "        std = [1.0, 1.0, 1.0],\n",
    "        max_pixel_value=255\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "validation_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.Normalize(\n",
    "        mean=[0.0, 0.0, 0.0],\n",
    "        std=[1.0, 1.0, 1.0],\n",
    "        max_pixel_value=255\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./datasets/training/training/images\\\\100_training.png', './datasets/training/training/images\\\\21_training.png', './datasets/training/training/images\\\\22_training.png', './datasets/training/training/images\\\\23_training.png', './datasets/training/training/images\\\\24_training.png', './datasets/training/training/images\\\\25_training.png', './datasets/training/training/images\\\\26_training.png', './datasets/training/training/images\\\\27_training.png', './datasets/training/training/images\\\\28_training.png', './datasets/training/training/images\\\\29_training.png', './datasets/training/training/images\\\\30_training.png', './datasets/training/training/images\\\\31_training.png', './datasets/training/training/images\\\\32_training.png', './datasets/training/training/images\\\\33_training.png', './datasets/training/training/images\\\\34_training.png', './datasets/training/training/images\\\\35_training.png', './datasets/training/training/images\\\\36_training.png', './datasets/training/training/images\\\\37_training.png', './datasets/training/training/images\\\\38_training.png', './datasets/training/training/images\\\\39_training.png', './datasets/training/training/images\\\\40_training.png', './datasets/training/training/images\\\\41_training.png', './datasets/training/training/images\\\\42_training.png', './datasets/training/training/images\\\\43_training.png', './datasets/training/training/images\\\\44_training.png', './datasets/training/training/images\\\\45_training.png', './datasets/training/training/images\\\\46_training.png', './datasets/training/training/images\\\\47_training.png', './datasets/training/training/images\\\\48_training.png', './datasets/training/training/images\\\\49_training.png', './datasets/training/training/images\\\\50_training.png', './datasets/training/training/images\\\\51_training.png', './datasets/training/training/images\\\\52_training.png', './datasets/training/training/images\\\\53_training.png', './datasets/training/training/images\\\\54_training.png', './datasets/training/training/images\\\\55_training.png', './datasets/training/training/images\\\\56_training.png', './datasets/training/training/images\\\\57_training.png', './datasets/training/training/images\\\\58_training.png', './datasets/training/training/images\\\\59_training.png', './datasets/training/training/images\\\\60_training.png', './datasets/training/training/images\\\\61_training.png', './datasets/training/training/images\\\\62_training.png', './datasets/training/training/images\\\\63_training.png', './datasets/training/training/images\\\\64_training.png', './datasets/training/training/images\\\\65_training.png', './datasets/training/training/images\\\\66_training.png', './datasets/training/training/images\\\\67_training.png', './datasets/training/training/images\\\\68_training.png', './datasets/training/training/images\\\\69_training.png', './datasets/training/training/images\\\\70_training.png', './datasets/training/training/images\\\\71_training.png', './datasets/training/training/images\\\\72_training.png', './datasets/training/training/images\\\\73_training.png', './datasets/training/training/images\\\\74_training.png', './datasets/training/training/images\\\\75_training.png', './datasets/training/training/images\\\\76_training.png', './datasets/training/training/images\\\\77_training.png', './datasets/training/training/images\\\\78_training.png', './datasets/training/training/images\\\\79_training.png', './datasets/training/training/images\\\\80_training.png', './datasets/training/training/images\\\\81_training.png', './datasets/training/training/images\\\\82_training.png', './datasets/training/training/images\\\\83_training.png', './datasets/training/training/images\\\\84_training.png', './datasets/training/training/images\\\\85_training.png', './datasets/training/training/images\\\\86_training.png', './datasets/training/training/images\\\\87_training.png', './datasets/training/training/images\\\\88_training.png', './datasets/training/training/images\\\\89_training.png', './datasets/training/training/images\\\\90_training.png', './datasets/training/training/images\\\\91_training.png', './datasets/training/training/images\\\\92_training.png', './datasets/training/training/images\\\\93_training.png', './datasets/training/training/images\\\\94_training.png', './datasets/training/training/images\\\\95_training.png', './datasets/training/training/images\\\\96_training.png', './datasets/training/training/images\\\\97_training.png', './datasets/training/training/images\\\\98_training.png', './datasets/training/training/images\\\\99_training.png']\n",
      "['./datasets/training/training/true_mask\\\\100_manual1.png', './datasets/training/training/true_mask\\\\21_manual1.gif', './datasets/training/training/true_mask\\\\22_manual1.gif', './datasets/training/training/true_mask\\\\23_manual1.gif', './datasets/training/training/true_mask\\\\24_manual1.gif', './datasets/training/training/true_mask\\\\25_manual1.gif', './datasets/training/training/true_mask\\\\26_manual1.gif', './datasets/training/training/true_mask\\\\27_manual1.gif', './datasets/training/training/true_mask\\\\28_manual1.gif', './datasets/training/training/true_mask\\\\29_manual1.gif', './datasets/training/training/true_mask\\\\30_manual1.gif', './datasets/training/training/true_mask\\\\31_manual1.gif', './datasets/training/training/true_mask\\\\32_manual1.gif', './datasets/training/training/true_mask\\\\33_manual1.gif', './datasets/training/training/true_mask\\\\34_manual1.gif', './datasets/training/training/true_mask\\\\35_manual1.gif', './datasets/training/training/true_mask\\\\36_manual1.gif', './datasets/training/training/true_mask\\\\37_manual1.gif', './datasets/training/training/true_mask\\\\38_manual1.gif', './datasets/training/training/true_mask\\\\39_manual1.gif', './datasets/training/training/true_mask\\\\40_manual1.gif', './datasets/training/training/true_mask\\\\41_manual1.png', './datasets/training/training/true_mask\\\\42_manual1.png', './datasets/training/training/true_mask\\\\43_manual1.png', './datasets/training/training/true_mask\\\\44_manual1.png', './datasets/training/training/true_mask\\\\45_manual1.png', './datasets/training/training/true_mask\\\\46_manual1.png', './datasets/training/training/true_mask\\\\47_manual1.png', './datasets/training/training/true_mask\\\\48_manual1.png', './datasets/training/training/true_mask\\\\49_manual1.png', './datasets/training/training/true_mask\\\\50_manual1.png', './datasets/training/training/true_mask\\\\51_manual1.png', './datasets/training/training/true_mask\\\\52_manual1.png', './datasets/training/training/true_mask\\\\53_manual1.png', './datasets/training/training/true_mask\\\\54_manual1.png', './datasets/training/training/true_mask\\\\55_manual1.png', './datasets/training/training/true_mask\\\\56_manual1.png', './datasets/training/training/true_mask\\\\57_manual1.png', './datasets/training/training/true_mask\\\\58_manual1.png', './datasets/training/training/true_mask\\\\59_manual1.png', './datasets/training/training/true_mask\\\\60_manual1.png', './datasets/training/training/true_mask\\\\61_manual1.png', './datasets/training/training/true_mask\\\\62_manual1.png', './datasets/training/training/true_mask\\\\63_manual1.png', './datasets/training/training/true_mask\\\\64_manual1.png', './datasets/training/training/true_mask\\\\65_manual1.png', './datasets/training/training/true_mask\\\\66_manual1.png', './datasets/training/training/true_mask\\\\67_manual1.png', './datasets/training/training/true_mask\\\\68_manual1.png', './datasets/training/training/true_mask\\\\69_manual1.png', './datasets/training/training/true_mask\\\\70_manual1.png', './datasets/training/training/true_mask\\\\71_manual1.png', './datasets/training/training/true_mask\\\\72_manual1.png', './datasets/training/training/true_mask\\\\73_manual1.png', './datasets/training/training/true_mask\\\\74_manual1.png', './datasets/training/training/true_mask\\\\75_manual1.png', './datasets/training/training/true_mask\\\\76_manual1.png', './datasets/training/training/true_mask\\\\77_manual1.png', './datasets/training/training/true_mask\\\\78_manual1.png', './datasets/training/training/true_mask\\\\79_manual1.png', './datasets/training/training/true_mask\\\\80_manual1.png', './datasets/training/training/true_mask\\\\81_manual1.png', './datasets/training/training/true_mask\\\\82_manual1.png', './datasets/training/training/true_mask\\\\83_manual1.png', './datasets/training/training/true_mask\\\\84_manual1.png', './datasets/training/training/true_mask\\\\85_manual1.png', './datasets/training/training/true_mask\\\\86_manual1.png', './datasets/training/training/true_mask\\\\87_manual1.png', './datasets/training/training/true_mask\\\\88_manual1.png', './datasets/training/training/true_mask\\\\89_manual1.png', './datasets/training/training/true_mask\\\\90_manual1.png', './datasets/training/training/true_mask\\\\91_manual1.png', './datasets/training/training/true_mask\\\\92_manual1.png', './datasets/training/training/true_mask\\\\93_manual1.png', './datasets/training/training/true_mask\\\\94_manual1.png', './datasets/training/training/true_mask\\\\95_manual1.png', './datasets/training/training/true_mask\\\\96_manual1.png', './datasets/training/training/true_mask\\\\97_manual1.png', './datasets/training/training/true_mask\\\\98_manual1.png', './datasets/training/training/true_mask\\\\99_manual1.png']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_x = sorted(glob(os.path.join(TRAIN_IMAGE_DIR, '*').replace(\"\\\\\", \"/\")))\n",
    "train_y = sorted(glob(os.path.join(TRAIN_MASK_DIR, '*').replace(\"\\\\\", \"/\")))\n",
    "\n",
    "valid_x = sorted(glob(os.path.join(VALID_IMAGE_DIR, '*').replace(\"\\\\\", \"/\")))\n",
    "valid_y = sorted(glob(os.path.join(VALID_MASK_DIR, '*').replace(\"\\\\\", \"/\")))\n",
    "\n",
    "print(train_x)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steve\\OneDrive\\Documents\\IntroToDL\\MLfun\\DRIVE Digital Retinal Images for Vessel Extraction\\main.ipynb Cell 7\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=0'>1</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m DRIVEDataset(train_x, train_y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=1'>2</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m DRIVEDataset(valid_x, valid_y)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=3'>4</a>\u001b[0m item \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(item[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n",
      "\u001b[1;32mc:\\Users\\steve\\OneDrive\\Documents\\IntroToDL\\MLfun\\DRIVE Digital Retinal Images for Vessel Extraction\\main.ipynb Cell 7\u001b[0m in \u001b[0;36mDRIVEDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=16'>17</a>\u001b[0m mask \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_dir[index], cv2\u001b[39m.\u001b[39mIMREAD_COLOR)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(mask))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=18'>19</a>\u001b[0m mask \u001b[39m=\u001b[39m mask \u001b[39m/\u001b[39;49m \u001b[39m255.0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=20'>21</a>\u001b[0m \u001b[39m#mask = np.expand_dims(mask, axis=0)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000019?line=21'>22</a>\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(mask, (\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "train_dataset = DRIVEDataset(train_x, train_y)\n",
    "valid_dataset = DRIVEDataset(valid_x, valid_y)\n",
    "\n",
    "item = train_dataset.__getitem__(1)\n",
    "plt.imshow(item[0].permute(1,2,0))\n",
    "plt.figure()\n",
    "plt.imshow(item[1].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A little trolling with double conv??</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A little trolling with network definition?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "        super(UNET, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # process Down-UNET:\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "            \n",
    "        #process Up-UNET:\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature * 2, feature, kernel_size=2, stride=2 \n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "        self.bottom = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels=out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottom(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        \n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "            \n",
    "            \n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_res = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_res)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A little pre-network testing??</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 161, 161])\n",
      "torch.Size([3, 1, 161, 161])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    x = torch.randn((3, 1, 161, 161))\n",
    "    model = UNET(in_channels=1, out_channels=1)\n",
    "    predictions = model(x)\n",
    "    \n",
    "    print(x.shape)\n",
    "    print(predictions.shape)\n",
    "    assert predictions.shape == x.shape\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Let's do some TRAINING????</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CHECKPT_PATH = './checkpoints/checkpoint.pth'\n",
    "print(f\"Using: {DEVICE}\")\n",
    "\n",
    "model = UNET().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, trainloader, optimizer, criterion):\n",
    "    \n",
    "    model.train()     \n",
    "    print(\"hey\")\n",
    "    for batch, (image, mask) in enumerate(trainloader):\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "        \n",
    "        output = model(image)\n",
    "        loss = criterion(output, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 202.00 MiB (GPU 0; 4.00 GiB total capacity; 3.43 GiB already allocated; 0 bytes free; 3.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steve\\OneDrive\\Documents\\IntroToDL\\MLfun\\DRIVE Digital Retinal Images for Vessel Extraction\\main.ipynb Cell 17\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=1'>2</a>\u001b[0m     train(device\u001b[39m=\u001b[39;49mDEVICE, model\u001b[39m=\u001b[39;49mmodel, trainloader\u001b[39m=\u001b[39;49mtrainloader, optimizer\u001b[39m=\u001b[39;49moptimizer, criterion\u001b[39m=\u001b[39;49mcriterion)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\steve\\OneDrive\\Documents\\IntroToDL\\MLfun\\DRIVE Digital Retinal Images for Vessel Extraction\\main.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(device, model, trainloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (image, mask) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=5'>6</a>\u001b[0m     image, mask \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mto(device), mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=7'>8</a>\u001b[0m     output \u001b[39m=\u001b[39m model(image)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=10'>11</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\steve\\anaconda3\\envs\\pytorchMain\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\steve\\OneDrive\\Documents\\IntroToDL\\MLfun\\DRIVE Digital Retinal Images for Vessel Extraction\\main.ipynb Cell 17\u001b[0m in \u001b[0;36mUNET.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=27'>28</a>\u001b[0m     x \u001b[39m=\u001b[39m down(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=28'>29</a>\u001b[0m     skip_connections\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=29'>30</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbottom(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steve/OneDrive/Documents/IntroToDL/MLfun/DRIVE%20Digital%20Retinal%20Images%20for%20Vessel%20Extraction/main.ipynb#ch0000016?line=32'>33</a>\u001b[0m skip_connections \u001b[39m=\u001b[39m skip_connections[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\steve\\anaconda3\\envs\\pytorchMain\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\steve\\anaconda3\\envs\\pytorchMain\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:162\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    163\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[0;32m    164\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[1;32mc:\\Users\\steve\\anaconda3\\envs\\pytorchMain\\lib\\site-packages\\torch\\_jit_internal.py:422\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    421\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\steve\\anaconda3\\envs\\pytorchMain\\lib\\site-packages\\torch\\nn\\functional.py:719\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[1;32m--> 719\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 202.00 MiB (GPU 0; 4.00 GiB total capacity; 3.43 GiB already allocated; 0 bytes free; 3.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(device=DEVICE, model=model, trainloader=trainloader, optimizer=optimizer, criterion=criterion)\n",
    "    print(f\"Epoch: {epoch}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorchMain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dea52dcf60439bbfeb15ff1bd40b703081e3cad41733c2dee57afaa83da4e6a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
